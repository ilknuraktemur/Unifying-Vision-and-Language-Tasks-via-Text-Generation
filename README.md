# Unifying-Vision-and-Language-Tasks-via-Text-Generation

In this study, we reproduce the works created in the paper named Unifying Vision-and-Language Tasks via Text Generation. Normally, there are task-specific models for each of these tasks in the literature.  But creating a separate model for each task requires extra effort and time. Moreover, the approach presented in this paper achieves success close to task specific models in the literature.

In this paper, 2 different models are presented that can perform different tasks on a single model. These are named VL-T5 and VL-BART. There are very minor architectural differences between the two. Our aim in this study is to set up experiments to prove that the model presented by the authors can deliver what they promise.
For this purpose, we create 5 different experiments.

https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation/blob/main/VQABiasDetection.ipynb -> Bias detection experiement

https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation/blob/main/VQAValid.ipynb -> Testing model experiment

https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation/blob/main/VQAValidError.ipynb -> Gender discrimination error experiment

https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation/blob/main/Reproducible_NLVR.ipynb -> Reproduciblity of NLVR task experiment

https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation/blob/main/Reproducible_VQA%20.ipynb -> Reproduciblity of VQA task experiment






